{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathsDataset(torch.utils.data.Dataset):\n",
    "    # init the dataset, shape = L x W\n",
    "    def __init__(self, path_dir, map_file, transform=None, shape = (100,100), device='cpu'):\n",
    "        self.device = device\n",
    "        self.paths = [] # create a list to hold all paths read from file\n",
    "        # self.map = np.loadtxt(map_file, skiprows=2).reshape(shape)\n",
    "        # self.map = self.map[np.newaxis, :, :]\n",
    "        x = torch.tensor([]) # empty list to hold input series tensors\n",
    "        num_paths = 0\n",
    "        for filename in tqdm(os.listdir(path_dir)):\n",
    "            num_paths += 1\n",
    "            with open(os.path.join(path_dir, filename), 'r') as f: # open in readonly mode\n",
    "                path_points_list = [] # a list to hold each point in a path\n",
    "                self.flat_path = np.loadtxt(f) # load in the flat path from file\n",
    "                self.path = np.asarray(self.flat_path, dtype=np.float32).reshape(len(self.flat_path)//2,2) #unflatten the path from the file\n",
    "                # print(self.path)\n",
    "                for point in self.path:\n",
    "                    x = point[0]\n",
    "                    y = point[1]\n",
    "                    this_point = [x, y]\n",
    "                    path_points_list.append(this_point)\n",
    "            sequence = torch.tensor(path_points_list, dtype=torch.float)[:, :]\n",
    "            self.paths.append(sequence)\n",
    "                # self.path_tensor = self.convert_path(shape, self.path)\n",
    "\n",
    "        # self.sequences = []\n",
    "        # for path in range(len(path_list)):\n",
    "        #    self.sequences.append((path_list[path] - mu)/sig)\n",
    "        \n",
    "        self.transform = transform\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def convert_path(self, map_dim, path):\n",
    "        path_mat = np.zeros(map_dim, dtype=float)\n",
    "\n",
    "        # Make the path continuous\n",
    "        for i in range(path.shape[0] - 1):\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            if (x1 < x2):\n",
    "                x_dir = 1\n",
    "            else:\n",
    "                x_dir = -1\n",
    "\n",
    "            if (y1 < y2):\n",
    "                y_dir = 1\n",
    "            else:\n",
    "                y_dir = -1\n",
    "\n",
    "            # Determine y from x\n",
    "            if x2-x1 != 0:\n",
    "                m = (y2-y1)/(x2-x1)\n",
    "                while x != x2:\n",
    "                    y = round(m*(x-x1) + y1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    x += x_dir\n",
    "            else:\n",
    "                while x != x2:\n",
    "                    path_mat[y1,x] = 1\n",
    "                    x += x_dir\n",
    "\n",
    "\n",
    "            x = path[i,0]\n",
    "            x1 = path[i,0]\n",
    "            x2 = path[i+1,0]\n",
    "\n",
    "            y = path[i,1]\n",
    "            y1 = path[i,1]\n",
    "            y2 = path[i+1,1]\n",
    "\n",
    "            # Determine x from y\n",
    "            if y2-y1 != 0:\n",
    "                m = (x2-x1)/(y2-y1)\n",
    "                while y != y2:\n",
    "                    x = round(m*(y-y1) + x1)\n",
    "                    path_mat[y,x] = 1\n",
    "                    y += y_dir\n",
    "            else:\n",
    "                while y != y2:\n",
    "                    path_mat[y,x1] = 1\n",
    "                    y += y_dir\n",
    "\n",
    "        # print(path)\n",
    "        # print(f'xs: {path[:,0]}')\n",
    "        # print(f'ys: {path[:,1]}')\n",
    "        # path_tensor = torch.tensor(path)\n",
    "        # print(path_tensor)\n",
    "        return path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x = np.float32(self.sequences[idx])\n",
    "        # x = torch.Tensor(x).to(self.device)\n",
    "\n",
    "        x = self.paths[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dataset name\n",
    "MAP_NAME = '8x12_map_cropped'\n",
    "DATASET = 'random_paths'\n",
    "MAP_SHAPE = (64,64)\n",
    "# training parameters\n",
    "BATCH_SIZE = 1\n",
    "train_dataset = PathsDataset(path_dir = f\"./env/{MAP_NAME}/paths/{DATASET}/dense\", map_file = f\"./env/{MAP_NAME}/{MAP_NAME}.txt\", shape = MAP_SHAPE, transform=None, device=device)\n",
    "dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 5, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 5, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),     \n",
    "            nn.Conv2d(64, 128, 7, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),  \n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new encoder\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: load encoder checkpoint from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: run the map through the encoder and get an encoded map to use in the lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_paths = []\n",
    "train_input_paths = []\n",
    "\n",
    "test_target_paths = []\n",
    "test_input_paths = []\n",
    "\n",
    "nbr_eval_paths = 10\n",
    "\n",
    "for path in train_dataset[nbr_eval_paths:]:\n",
    "    input_path = path[:-1] #all points execpt last\n",
    "    target_path = path[1:]\n",
    "\n",
    "    train_target_paths.append(target_path)\n",
    "    train_input_paths.append(input_path)\n",
    "\n",
    "for path in train_dataset[:nbr_eval_paths]:\n",
    "    input_path = path[:-1] #all points execpt last five\n",
    "    target_path = path[1:]\n",
    "\n",
    "    test_target_paths.append(target_path)\n",
    "    test_input_paths.append(input_path)\n",
    "\n",
    "\n",
    "print(len(train_input_paths))\n",
    "print(len(test_input_paths))\n",
    "train_input_paths[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "  # hidden_d - the size of the hidden LSTM layers\n",
    "  # map_d - the flattened/encoded map dimension\n",
    "  def __init__(self, hidden_d=120, map_d=100):\n",
    "    self.hidden_d = hidden_d\n",
    "    super(MyLSTM, self).__init__()\n",
    "\n",
    "    # map hidden layer\n",
    "    self.lstm_map = nn.LSTMCell(input_size=map_d, hidden_size=hidden_d)\n",
    "\n",
    "    # points hidden layer\n",
    "    self.lstm_points = nn.LSTMCell(input_size=4, hidden_size=hidden_d)\n",
    "    \n",
    "    # \"upper\" hidden layer\n",
    "    self.lstm1 = nn.LSTMCell(input_size=hidden_d*2, hidden_size=hidden_d)\n",
    "    self.fc = nn.Linear(hidden_d, 2)\n",
    "\n",
    "\n",
    "  def forward(self, goal_point, current_point, map, future=0):\n",
    "\n",
    "    \n",
    "\n",
    "    # Creation of cell state and hidden state for map hidden layer\n",
    "    hidden_state_map = torch.zeros(1, self.hidden_d)\n",
    "    cell_state_map = torch.zeros(1, self.hidden_d)\n",
    "\n",
    "    # Creation of cell state and hidden state for points hidden layer\n",
    "    hidden_state_points = torch.zeros(1, self.hidden_d)\n",
    "    cell_state_points = torch.zeros(1, self.hidden_d)\n",
    "\n",
    "    # Creation of cell state and hidden state for \"upper\" hidden layer\n",
    "    hidden_state_1 = torch.zeros(1, self.hidden_d)\n",
    "    cell_state_1 = torch.zeros(1, self.hidden_d)\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "\n",
    "    # initialize weights to random[-0.1, 0.1) (need to update initialzation to match paper)\n",
    "    # weights initialization\n",
    "    torch.nn.init.xavier_normal_(hidden_state_map)\n",
    "    torch.nn.init.xavier_normal_(cell_state_map)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(hidden_state_points)\n",
    "    torch.nn.init.xavier_normal_(cell_state_points)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(hidden_state_1)\n",
    "    torch.nn.init.xavier_normal_(cell_state_1)\n",
    "\n",
    "    # Concatenate start and goal\n",
    "    points = torch.cat([current_point, goal_point], 0)\n",
    "\n",
    "    hidden_state_map, cell_state_map = self.lstm1(map, (hidden_state_map, cell_state_map))\n",
    "\n",
    "    hidden_state_points, cell_state_points = self.lstm1(points, (hidden_state_points, cell_state_points))\n",
    "\n",
    "    # Concatenate the output the lstm layer output from points and map into a single input to the final \"upper\" hidden layer\n",
    "    final_layer_input = torch.cat([hidden_state_map, hidden_state_points], 0)\n",
    "    hidden_state_1, cell_state_1 = self.lstm1(final_layer_input, (hidden_state_1, cell_state_1))\n",
    "      \n",
    "    # Last hidden state is passed through a fully connected neural net\n",
    "    output = self.fc(hidden_state_1)\t\n",
    "    # print(f'out: {output}')\n",
    "    outputs.append(output)\n",
    "\n",
    "    for i  in range(future): # if we are trying to predict future values (if future is not zero)\n",
    "      hidden_state_1, cell_state_1 = self.lstm1(x[i:i+1], (hidden_state_1, cell_state_1))\n",
    "      \n",
    "    # Last hidden state is passed through a fully connected neural net\n",
    "      output = self.fc(hidden_state_1)\t\n",
    "      # print(f'out: {output}')\n",
    "      outputs.append(output)\n",
    "\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = MyLSTM()\n",
    "loss = []\n",
    "criterion = nn.MSELoss()\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "# n_epochs = 1000\n",
    "n_epochs = 1\n",
    "for e in range(n_epochs):\n",
    "  print(f'epoch: {e}')\n",
    "  for s in range(len(train_input_paths)):\n",
    "    # print(train_input_paths[s].shape)\n",
    "    pred = rnn(train_input_paths[s])  # predict next step, init hidden state to zero at the begining of the sequence\n",
    "    # print(f'pred: {pred.shape}')\n",
    "    # print(f'target: {train_target_paths[s].shape}')\n",
    "    err = criterion(pred, train_target_paths[s])  # predict next step for each step\n",
    "    opt.zero_grad()\n",
    "    err.backward()\n",
    "    opt.step()\n",
    "    loss.append(err.item())\n",
    "    if s % 100 is 0:  \n",
    "      print(err.item())\n",
    "plt.plot(loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_map = np.loadtxt(\"./env/8x12_map_cropped/8x12_map_cropped.txt\", skiprows=2)\n",
    "map = np.asarray(flat_map).reshape(64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction loop\n",
    "i = 3 # what path # from the eval dataset we want to look at\n",
    "with torch.no_grad():\n",
    "    future=15\n",
    "    length = train_input_paths[i].shape[0]\n",
    "    pred = rnn(train_input_paths[i], future=future)\n",
    "    truth = train_target_paths[i]\n",
    "    # loss = loss_func(pred[:, :-future], test_target)\n",
    "    # print(f'loss: {loss.item()}')\n",
    "    y = pred.detach().numpy()\n",
    "print(length)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_input_paths[3])\n",
    "# print(start_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = y[:,0]\n",
    "ys = y[:,1]\n",
    "\n",
    "plt.imshow(map)\n",
    "plt.plot(xs[:length], ys[:length], color='k', label = \"input\")\n",
    "plt.plot(xs[length:], ys[length:], color='b', label = \"predicted\")\n",
    "plt.plot(truth[:,0], truth[:,1], color='r', label = \"truth\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_points_list = []\n",
    "coords_list = []\n",
    "\n",
    "num_points = 150 # num points to predict for each path\n",
    "paths_to_gen = 6 # num paths to generate\n",
    "\n",
    "for x in range(paths_to_gen):\n",
    "    x = random.randint(0, 64)\n",
    "    y = random.randint(0, 64)\n",
    "    start_point = torch.from_numpy(np.asarray([[x,y]])).float()\n",
    "    points = []\n",
    "    with torch.no_grad():\n",
    "        pred = rnn(start_point, future=0)\n",
    "        points.append(pred)\n",
    "        for point in range(num_points):\n",
    "            pred = rnn(pred, future=0)\n",
    "            points.append(pred)\n",
    "        points = torch.cat(points, dim=0)\n",
    "\n",
    "    coords_list.append(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "flat_map = np.loadtxt(\"./env/8x12_map_cropped/8x12_map_cropped.txt\", skiprows=2)\n",
    "map = np.asarray(flat_map).reshape(64,64)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=5, ncols=1, figsize=(25, 25))\n",
    "for x in range(5):\n",
    "    ax[x].scatter(coords_list[x][:,0], coords_list[x][:,1], label='predicted points')\n",
    "    ax[x].scatter(coords_list[x][0,0], coords_list[x][0,1], label='start point')\n",
    "    ax[x].legend()\n",
    "    ax[x].imshow(map)\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bayesianNN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65c0cf972fe55eaf0c962c4929f592d86a72c532b00283f932a90435beee88e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
